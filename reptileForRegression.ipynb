{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arpPm0c8-xJP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.keras.backend as keras_backend\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjdNQVA3w8wa"
      },
      "outputs": [],
      "source": [
        "class DataGenerator():\n",
        "    def __init__(self, K=10, alfa=None, beta=None):\n",
        "        self.K = K\n",
        "        self.amplitude = alfa if alfa else np.random.uniform(20.0, 50.0)/100\n",
        "        self.beta = beta if beta else np.random.uniform(0.0, 30.0)/100\n",
        "        self.sampled_points = None\n",
        "        self.x1 = self._sample()\n",
        "        self.x2 = self._sample()\n",
        "        self.x3 = self._sample()\n",
        "        self.x4 = self._sample()\n",
        "        self.x5 = self._sample()\n",
        "\n",
        "    def _sample(self):\n",
        "        return np.random.uniform(0.0, 100.0, self.K)/100\n",
        "\n",
        "    def f(self, x1, x2, x3, x4, x5):\n",
        "        y1 = x2*(1 + (x1+self.beta)*self.amplitude) #потребление\n",
        "        y2 = x3*(1 + (x1+x2+x4+x5+self.beta)*self.amplitude) #производство\n",
        "        y3 = abs(y2-y1+self.beta) #импорт\n",
        "        y4 = abs(y2+y3-y1+self.beta) #экспорт\n",
        "        return y1*1000, y2*1000, y3*10, y4*10\n",
        "\n",
        "    def batch(self, x1 = None, x2 = None, x3 = None, x4 = None, x5 = None):\n",
        "        if x1 is None:\n",
        "            x1 = self.x1\n",
        "        if x2 is None:\n",
        "            x2 = self.x2\n",
        "        if x3 is None:\n",
        "            x3 = self.x3\n",
        "        if x4 is None:\n",
        "            x4 = self.x4\n",
        "        if x5 is None:\n",
        "            x5 = self.x5\n",
        "        y1, y2, y3, y4 = self.f(x1, x2, x3, x4, x5)\n",
        "        x = []\n",
        "        y = []\n",
        "        for i in range(0,len(x1)):\n",
        "            temp_x = [x1[i], x2[i], x3[i], x4[i], x5[i]]\n",
        "            x.append(temp_x)\n",
        "        for i in  range(0,len(y1)):\n",
        "            temp_y = [y1[i], y2[i], y3[i], y4[i]]\n",
        "            y.append(temp_y)\n",
        "        return x, y\n",
        "\n",
        "    def equally_spaced_samples(self, K=None):\n",
        "        '''Returns `K` equally spaced samples.'''\n",
        "        if K is None:\n",
        "            K = self.K\n",
        "        return self.batch(\n",
        "            x1=np.linspace(1, 99, K)/100,\n",
        "            x2=np.linspace(1, 99, K)/100,\n",
        "            x3=np.linspace(1, 99, K)/100,\n",
        "            x4=np.linspace(1, 99, K)/100,\n",
        "            x5=np.linspace(1, 99, K)/100,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batches = []\n",
        "for i in range(2):\n",
        "    batches.append(DataGenerator(K=3).equally_spaced_samples())\n",
        "    (x,y) = batches[i]\n",
        "    print(np.array(x))\n",
        "    print(np.array(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSM4BmUgJJ1h"
      },
      "outputs": [],
      "source": [
        "class MyModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden1 = keras.layers.Dense(256, input_shape=(5,))\n",
        "        self.hidden2 = keras.layers.Dense(128)\n",
        "        self.hidden3 = keras.layers.Dense(64)\n",
        "        self.out = keras.layers.Dense(4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = keras.activations.relu(self.hidden1(x))\n",
        "        x = keras.activations.relu(self.hidden2(x))\n",
        "        x = keras.activations.relu(self.hidden3(x))\n",
        "        x = keras.activations.linear(self.out(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GAXW-MLSrzp"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def root_mean_squared_error(model, x, y):\n",
        "        return K.sqrt(K.mean(K.square(model.forward(x) - y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMcxkBivJVvb"
      },
      "outputs": [],
      "source": [
        "training = []\n",
        "testing = []\n",
        "\n",
        "def reptile(model, train_ds, test_ds, epochs, meta_step_size = 0.15, eval_interval = 1, show_step = 100):\n",
        "  optimizer = keras.optimizers.Adam()\n",
        "  total_test_loss_arr = []\n",
        "  total_train_loss_arr = []\n",
        "  for epo in range(epochs):\n",
        "      test_loss_arr = []\n",
        "      train_loss_arr = []\n",
        "      start = time.time()\n",
        "      frac_done = epo/epochs\n",
        "      cur_meta_step_size = (1 - frac_done) * meta_step_size\n",
        "      print(cur_meta_step_size)\n",
        "      if len(model.get_weights()) == 0:\n",
        "          x,y = train_ds[0].batch()\n",
        "          x = np.array(x)\n",
        "          model.forward(x)\n",
        "\n",
        "      old_vars = model.get_weights()\n",
        "\n",
        "      for i, t in enumerate(random.sample(range(0, len(train_ds)), len(train_ds))):\n",
        "          x,y = train_ds[t].batch()\n",
        "          x = np.array(x)\n",
        "          y = np.array(y)\n",
        "          with tf.GradientTape() as tape:\n",
        "              preds = model.forward(x)\n",
        "              loss = keras.losses.MSE(y, preds)\n",
        "          grads = tape.gradient(loss, model.trainable_weights)\n",
        "          optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "      new_vars = model.get_weights()\n",
        "\n",
        "      for var in range(len(new_vars)):\n",
        "          new_vars[var] = old_vars[var] + (\n",
        "              (new_vars[var] - old_vars[var]) * cur_meta_step_size\n",
        "          )\n",
        "\n",
        "      model.set_weights(new_vars)\n",
        "\n",
        "      for i, t in enumerate(random.sample(range(0, len(train_ds)), len(train_ds))):\n",
        "          x,y = train_ds[t].batch()\n",
        "          x = np.array(x)\n",
        "          y = np.array(y)\n",
        "          train_loss = root_mean_squared_error(model, x, y)\n",
        "          train_loss_arr.append(train_loss)\n",
        "\n",
        "      # Evaluation loop\n",
        "      for _, t in enumerate(random.sample(range(0, len(test_ds)), len(test_ds))):\n",
        "          x,y = train_ds[t].batch()\n",
        "          x = np.array(x)\n",
        "          y = np.array(y)\n",
        "          test_loss = root_mean_squared_error(model, x, y)\n",
        "          test_loss_arr.append(test_loss)\n",
        "      print('Epo {}: loss_train = {}, loss_test = {}, Time to run {}'\n",
        "        .format(epo, sum(train_loss_arr) / len(train_loss_arr), sum(test_loss_arr) / len(test_loss_arr), time.time() - start))\n",
        "      total_test_loss_arr.append(sum(test_loss_arr) / len(test_loss_arr))\n",
        "      total_train_loss_arr.append(sum(train_loss_arr) / len(train_loss_arr))\n",
        "  plt.plot(total_train_loss_arr)\n",
        "  plt.plot(total_test_loss_arr)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQpbSacd7XAA"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(K, train_size=1000, test_size=100):\n",
        "    def _generate_dataset(size):\n",
        "        return [DataGenerator(K=K) for _ in range(size)]\n",
        "    return _generate_dataset(train_size), _generate_dataset(test_size)\n",
        "\n",
        "train_ds, test_ds = generate_dataset(K=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-72y7CT97Ox_"
      },
      "outputs": [],
      "source": [
        "rept = MyModel()\n",
        "reptile(rept, train_ds, test_ds, 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u523HQBk2DRo"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('./data/fin.csv', encoding='latin-1')\n",
        "train_data = train_df.drop(columns = ['Country'])\n",
        "\n",
        "train_data_arr = []\n",
        "train_targets_arr = []\n",
        "\n",
        "test_data_arr = []\n",
        "test_target_arr = []\n",
        "r = random.randint(0,10)\n",
        "k=0\n",
        "valueRange = 0\n",
        "\n",
        "for i in range(1,len(train_data)):\n",
        "\n",
        "  for j in range(i-1,i - valueRange - 1,-1):\n",
        "    if(j<=0):\n",
        "      continue\n",
        "    data_row = train_data[i-1:i]\n",
        "    data_row.at[i-1, 'Year'] = 1960 + j\n",
        "    if k == 11:\n",
        "      r = random.randint(0,10)\n",
        "      k=0\n",
        "    if k == r:\n",
        "      test_data_arr.append(data_row.copy())\n",
        "      target_row = train_data[j-1:j]\n",
        "      test_target_arr.append(target_row.drop(columns = ['Year']))\n",
        "      k+=1\n",
        "      continue\n",
        "    k+=1\n",
        "    train_data_arr.append(data_row.copy())\n",
        "    target_row = train_data[j-1:j]\n",
        "    train_targets_arr.append(target_row.drop(columns = ['Year']))\n",
        "\n",
        "  for j in range(i,len(train_data)):\n",
        "    data_row = train_data[i-1:i]\n",
        "    data_row.at[i-1, 'Year'] = 1960 + j\n",
        "    if k == 11:\n",
        "      r = random.randint(0,10)\n",
        "      k=0\n",
        "    if k == r:\n",
        "      test_data_arr.append(data_row.copy())\n",
        "      target_row = train_data[j:j+1]\n",
        "      test_target_arr.append(target_row.drop(columns = ['Year']))\n",
        "      k+=1\n",
        "      continue\n",
        "    k+=1\n",
        "    train_data_arr.append(data_row.copy())\n",
        "    target_row = train_data[j:j+1]\n",
        "    train_targets_arr.append(target_row.drop(columns = ['Year']))\n",
        "    if j-i > valueRange-1:\n",
        "      break\n",
        "\n",
        "train_targets = pd.concat(train_targets_arr)\n",
        "train_data = pd.concat(train_data_arr)\n",
        "\n",
        "test_targets = pd.concat(test_target_arr)\n",
        "test_data = pd.concat(test_data_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgwiRT5OaO2c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_data)\n",
        "scaled_features = scaler.transform(train_data)\n",
        "train_data = pd.DataFrame(data = scaled_features)\n",
        "\n",
        "scaled_features = scaler.transform(test_data)\n",
        "test_data = pd.DataFrame(data = scaled_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K936L6CaRCj"
      },
      "outputs": [],
      "source": [
        "train_data_np = train_data.to_numpy()\n",
        "train_targets_np = train_targets.to_numpy()\n",
        "\n",
        "test_data_np = test_data.to_numpy()\n",
        "test_targets_np = test_targets.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjKtQw5h8Q5U"
      },
      "outputs": [],
      "source": [
        "def MiniBatches(X, Y, MiniBatchSize):\n",
        "\n",
        "    m = X.shape[0]\n",
        "    miniBatches = []\n",
        "\n",
        "    num_minibatches = m // MiniBatchSize\n",
        "    for k in range(0, num_minibatches):\n",
        "        miniBatch_X = X[k * MiniBatchSize:(k + 1) * MiniBatchSize,:]\n",
        "        miniBatch_Y = Y[k * MiniBatchSize:(k + 1) * MiniBatchSize,:]\n",
        "        miniBatch = (miniBatch_X, miniBatch_Y)\n",
        "        miniBatches.append(miniBatch)\n",
        "\n",
        "\n",
        "    if m % MiniBatchSize != 0:\n",
        "        miniBatch_X = X[num_minibatches * MiniBatchSize:, :]\n",
        "        miniBatch_Y = Y[num_minibatches * MiniBatchSize:, :]\n",
        "\n",
        "        miniBatch = (miniBatch_X, miniBatch_Y)\n",
        "        miniBatches.append(miniBatch)\n",
        "\n",
        "    return miniBatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AleTrjfJf7my"
      },
      "outputs": [],
      "source": [
        "def copy_model(model, x):\n",
        "    copied_model = MyModel()\n",
        "    copied_model.forward(x)\n",
        "    copied_model.set_weights(model.get_weights())\n",
        "    return copied_model\n",
        "\n",
        "test_loss_arr = []\n",
        "train_loss_arr = []\n",
        "\n",
        "def loss_function(pred_y, y):\n",
        "  return keras_backend.mean(keras.losses.MAE(y, pred_y))\n",
        "\n",
        "def compute_loss(model, x, y, loss_fn=loss_function):\n",
        "    logits = model.forward(x)\n",
        "    mae = loss_fn(logits, y)\n",
        "    return mae, logits\n",
        "\n",
        "def compute_gradients(model, x, y, loss_fn=loss_function):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss, _ = compute_loss(model, x, y, loss_fn)\n",
        "    return tape.gradient(loss, model.trainable_variables), loss\n",
        "\n",
        "\n",
        "def train_batch(x, y, model, optimizer):\n",
        "    gradients, loss = compute_gradients(model, x, y)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "def eval_test(model, optimizer, x_train, y_train, x_test, y_test, num_steps=(0, 1, 10)):\n",
        "    fit_res = []\n",
        "\n",
        "    if 0 in num_steps:\n",
        "        loss, logits = compute_loss(model, x_train, y_train)\n",
        "        print(loss)\n",
        "\n",
        "    for step in range(1, np.max(num_steps) + 1):\n",
        "        batch = MiniBatches(x_train, y_train, 2)\n",
        "        for i, t in enumerate(random.sample(range(0, len(batch)), len(batch))):\n",
        "            x_batch,y_batch = batch[t]\n",
        "            train_batch(x_batch, y_batch, model, optimizer)\n",
        "        loss_test = root_mean_squared_error(model, x_test, y_test)\n",
        "        loss_train = root_mean_squared_error(model, x_train, y_train)\n",
        "        print('Epoh {}:loss_test = {} loss_train = {}'.format(step, loss_test, loss_train))\n",
        "        test_loss_arr.append(loss_test)\n",
        "        train_loss_arr.append(loss_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "def eval_for_test(model, train_data_np, train_targets_np, test_data, test_targets,  num_steps=(0, 1, 10), lr=0.005):\n",
        "\n",
        "    copied_model = copy_model(model, train_data_np)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "\n",
        "    model = eval_test(copied_model, optimizer, train_data_np, train_targets_np, test_data, test_targets, num_steps)\n",
        "    plt.plot(test_loss_arr)\n",
        "    plt.plot(train_loss_arr)\n",
        "    plt.show()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcSvCg8NxZhj"
      },
      "outputs": [],
      "source": [
        "model = eval_for_test(rept, train_data_np, train_targets_np, test_data_np, test_targets_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTIG_UvVhVYE"
      },
      "outputs": [],
      "source": [
        "data = {'Year': [2017], 'Eggs Food': [2278.0],\n",
        "        'Eggs Production': [2444.0], 'Eggs Import Quantity': [98.0],\n",
        "       'Eggs Export Quantity': [18.0] }\n",
        "\n",
        "result = {'Eggs Food': [2410.52],\n",
        "          'Eggs Production': [2519.0],\n",
        "          'Eggs Import Quantity': [97.0],\n",
        "          'Eggs Export Quantity': [36.0]}\n",
        "\n",
        "input_data = pd.DataFrame(data)\n",
        "scaled_features = scaler.transform(input_data)\n",
        "input_data = pd.DataFrame(data = scaled_features)\n",
        "input_data_np = input_data.to_numpy()\n",
        "\n",
        "input_data = pd.DataFrame(result)\n",
        "result_data = pd.DataFrame(data = input_data)\n",
        "result_data_np = result_data.to_numpy()\n",
        "\n",
        "pred = model.forward(input_data_np)\n",
        "print(pred)\n",
        "print(result_data_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekr9kidjOU7w"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "pred = model.forward(test_data_np)\n",
        "print(mean_absolute_percentage_error(pred, test_targets_np))\n",
        "print(pred)\n",
        "print(test_targets_np)\n",
        "print(pred/test_targets_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка на данных за последние несколько лет"
      ],
      "metadata": {
        "id": "9wYRzIeJ5v6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./data/fin.csv', encoding='latin-1')\n",
        "train_data = train_df.drop(columns = ['Country']).tail(5)\n",
        "\n",
        "last_year_data_arr = []\n",
        "last_year_targets_arr = []\n",
        "\n",
        "valueRange = 0\n",
        "\n",
        "for i in range(1,len(train_data)):\n",
        "\n",
        "  for j in range(i,len(train_data)):\n",
        "    data_row = train_data.head(i).tail(1)\n",
        "    data_row.at[i+25, 'Year'] = (int)(2015 + j)\n",
        "    last_year_data_arr.append(data_row.copy())\n",
        "    target_row = train_data[j:j+1]\n",
        "    last_year_targets_arr.append(target_row.drop(columns = ['Year']))\n",
        "    if j-i > valueRange-1:\n",
        "      break\n",
        "\n",
        "last_year_targets = pd.concat(last_year_targets_arr)\n",
        "last_year_data = pd.concat(last_year_data_arr)\n",
        "\n",
        "scaler2 = MinMaxScaler()\n",
        "scaler2.fit(last_year_data)\n",
        "scaled_features2 = scaler2.transform(last_year_data)\n",
        "last_year_data = pd.DataFrame(data = scaled_features2)\n",
        "\n",
        "last_year_data_np = last_year_data.to_numpy()\n",
        "last_year_targets_np = last_year_targets.to_numpy()"
      ],
      "metadata": {
        "id": "GQ9LUZJV50KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "pred = model.forward(last_year_data_np)\n",
        "print(mean_absolute_percentage_error(pred, last_year_targets_np))\n",
        "print(pred)\n",
        "print(last_year_targets_np)\n",
        "print(pred/last_year_targets_np)"
      ],
      "metadata": {
        "id": "VA_H7m8w7Gbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}