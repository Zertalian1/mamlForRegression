{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rVKpLhSVYM6L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.backend as keras_backend\n",
        "\n",
        "\n",
        "# Other dependencies\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproduction\n",
        "np.random.seed(322)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpojjSDvJX1f"
      },
      "outputs": [],
      "source": [
        "class DataGenerator():\n",
        "    def __init__(self, K=10, alfa=None, beta=None):\n",
        "        self.K = K\n",
        "        self.amplitude = alfa if alfa else np.random.uniform(20.0, 50.0)/100\n",
        "        self.beta = beta if beta else np.random.uniform(1.0, 30.0)/100\n",
        "        self.sampled_points = None\n",
        "        self.x1 = self._sample()\n",
        "        self.x2 = self._sample()\n",
        "        self.x3 = self._sample()\n",
        "        self.x4 = self._sample()\n",
        "        self.x5 = self._sample()\n",
        "\n",
        "    def _sample(self):\n",
        "        return np.random.uniform(0.0, 100.0, self.K)/100\n",
        "\n",
        "    def f(self, x1, x2, x3, x4, x5):\n",
        "        y1 = x2*(1 + (x1+self.beta)*self.amplitude) #потребление\n",
        "        y2 = x3*(1 + (x1+x2+x4+x5+self.beta)*self.amplitude) #производство\n",
        "        y3 = abs(y2-y1+self.beta) #импорт\n",
        "        y4 = abs(y2+y3-y1+self.beta) #экспорт\n",
        "        return y1*1000, y2*1000, y3*10, y4*10\n",
        "\n",
        "    def batch(self, x1 = None, x2 = None, x3 = None, x4 = None, x5 = None):\n",
        "        if x1 is None:\n",
        "            x1 = self.x1\n",
        "        if x2 is None:\n",
        "            x2 = self.x2\n",
        "        if x3 is None:\n",
        "            x3 = self.x3\n",
        "        if x4 is None:\n",
        "            x4 = self.x4\n",
        "        if x5 is None:\n",
        "            x5 = self.x5\n",
        "        y1, y2, y3, y4 = self.f(x1, x2, x3, x4, x5)\n",
        "        x = []\n",
        "        y = []\n",
        "        for i in range(0,len(x1)):\n",
        "            temp_x = [x1[i], x2[i], x3[i], x4[i], x5[i]]\n",
        "            x.append(temp_x)\n",
        "        for i in  range(0,len(y1)):\n",
        "            temp_y = [y1[i], y2[i], y3[i], y4[i]]\n",
        "            y.append(temp_y)\n",
        "        return x, y\n",
        "\n",
        "    def equally_spaced_samples(self, K=None):\n",
        "        '''Returns `K` equally spaced samples.'''\n",
        "        if K is None:\n",
        "            K = self.K\n",
        "        return self.batch(\n",
        "            x1=np.linspace(1, 99, K)/100,\n",
        "            x2=np.linspace(1, 99, K)/100,\n",
        "            x3=np.linspace(1, 99, K)/100,\n",
        "            x4=np.linspace(1, 99, K)/100,\n",
        "            x5=np.linspace(1, 99, K)/100,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batches = []\n",
        "for i in range(2):\n",
        "    batches.append(DataGenerator(K=3).equally_spaced_samples())\n",
        "    (x,y) = batches[i]\n",
        "    print(x)\n",
        "    print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1uAQb_UhYeym"
      },
      "outputs": [],
      "source": [
        "class MyModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden1 = keras.layers.Dense(256, input_shape=(5,))\n",
        "        self.hidden2 = keras.layers.Dense(128)\n",
        "        self.hidden3 = keras.layers.Dense(64)\n",
        "        self.out = keras.layers.Dense(4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = keras.activations.relu(self.hidden1(x))\n",
        "        x = keras.activations.relu(self.hidden2(x))\n",
        "        x = keras.activations.relu(self.hidden3(x))\n",
        "        x = keras.activations.linear(self.out(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AknNYczD8FpB"
      },
      "outputs": [],
      "source": [
        "def copy_model(model, x):\n",
        "    copied_model = MyModel()\n",
        "    copied_model.forward(x)\n",
        "    copied_model.set_weights(model.get_weights())\n",
        "    return copied_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XbUKfwhu22CK"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def root_mean_squared_error(model, x, y):\n",
        "        return K.sqrt(K.mean(K.square(model.forward(x) - y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3HuLsZ8vaMHJ"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(K, train_size=10000, test_size=600):\n",
        "    def _generate_dataset(size):\n",
        "        return [DataGenerator(K=K) for _ in range(size)]\n",
        "    return _generate_dataset(train_size), _generate_dataset(test_size)\n",
        "\n",
        "train_ds, test_ds = generate_dataset(K=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def TaskBatches(X, BatchSize):\n",
        "\n",
        "    m = len(X)\n",
        "    miniBatches = []\n",
        "\n",
        "    num_batches = m // BatchSize\n",
        "    for k in range(0, num_batches):\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for i in range(0,BatchSize):\n",
        "          (x,y) = X[i].batch()\n",
        "          batch_x.append(x[0])\n",
        "          batch_y.append(y[0])\n",
        "        miniBatches.append([batch_x,batch_y])\n",
        "\n",
        "    if m % BatchSize != 0:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for i in range(0, m%BatchSize):\n",
        "          (x,y) = X[i].batch()\n",
        "          batch_x.append(x)\n",
        "          batch_y.append(x)\n",
        "        miniBatches.append((batch_x,batch_y))\n",
        "\n",
        "    return miniBatches"
      ],
      "metadata": {
        "id": "CJC-FsrzyzSs"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(K, train_size=4):\n",
        "    def _generate_dataset(size):\n",
        "        return [DataGenerator(K=K) for _ in range(size)]\n",
        "    return _generate_dataset(train_size)\n",
        "\n",
        "tasks = generate_dataset(K=1)\n",
        "\n",
        "task_batches = TaskBatches(tasks,2)\n",
        "print(task_batches)"
      ],
      "metadata": {
        "id": "nCo1HVudzG22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "_6ynNxdvdoQk"
      },
      "outputs": [],
      "source": [
        "def loss_function(pred_y, y):\n",
        "  return keras_backend.mean(keras.losses.MAE(y, pred_y))\n",
        "\n",
        "def compute_loss(model, x, y, loss_fn=loss_function):\n",
        "    logits = model.forward(x)\n",
        "    mae = loss_fn(logits, y)\n",
        "    return mae, logits\n",
        "\n",
        "def train_maml(model, train_ds, test_ds, epochs, lr_inner=0.1, show_step = 100):\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    total_test_loss_arr = []\n",
        "    total_train_loss_arr = []\n",
        "    for epo in range(epochs):\n",
        "        test_loss_arr = []\n",
        "        train_loss_arr = []\n",
        "        start = time.time()\n",
        "        steps_loss = []\n",
        "        task_batches = TaskBatches(train_ds,2)\n",
        "        for i, t in enumerate(random.sample(range(0, len(task_batches)), len(task_batches))):\n",
        "            lr = lr_inner\n",
        "            x = task_batches[t][0]\n",
        "            y = task_batches[t][1]\n",
        "            x = np.array(x)\n",
        "            y = np.array(y)\n",
        "\n",
        "            if len(model.get_weights()) == 0:\n",
        "                model.forward(x)\n",
        "            with tf.GradientTape() as test_tape:\n",
        "\n",
        "                with tf.GradientTape() as train_tape:\n",
        "                    loss, _ = compute_loss(model, x, y)\n",
        "                    gradients = train_tape.gradient(loss, model.trainable_variables)\n",
        "                k = 0\n",
        "                model_copy = copy_model(model, x)\n",
        "                for j in range(len(model_copy.layers)):\n",
        "                    model_copy.layers[j].kernel = tf.subtract(model.layers[j].kernel, tf.multiply(lr, gradients[k]))\n",
        "                    model_copy.layers[j].bias = tf.subtract(model.layers[j].bias, tf.multiply(lr, gradients[k+1]))\n",
        "                    k += 2\n",
        "\n",
        "                loss, _ = compute_loss(model_copy, x, y)\n",
        "\n",
        "                gradients = test_tape.gradient(loss, model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "                steps_loss.append(loss)\n",
        "                if i%show_step == 0:\n",
        "                  print('Step {}: loss_train = {}'.format(i,sum(steps_loss) / len(steps_loss)))\n",
        "\n",
        "        for i, t in enumerate(random.sample(range(0, len(train_ds)), len(train_ds))):\n",
        "            x, y = train_ds[t].batch()\n",
        "            x = np.array(x)\n",
        "            y = np.array(y)\n",
        "            test_loss = root_mean_squared_error(model, x, y)\n",
        "            train_loss_arr.append(test_loss)\n",
        "\n",
        "        for i, t in enumerate(random.sample(range(0, len(test_ds)), len(test_ds))):\n",
        "          x, y = train_ds[t].batch()\n",
        "          x = np.array(x)\n",
        "          y = np.array(y)\n",
        "          loss = root_mean_squared_error(model, x, y)\n",
        "          test_loss_arr.append(loss)\n",
        "\n",
        "\n",
        "        print('Epo {}: loss_train = {}, loss_test = {}, Time to run {}'\n",
        "        .format(epo, sum(train_loss_arr) / len(train_loss_arr), sum(test_loss_arr) / len(test_loss_arr), time.time() - start))\n",
        "        total_test_loss_arr.append(sum(test_loss_arr) / len(test_loss_arr))\n",
        "        total_train_loss_arr.append(sum(train_loss_arr) / len(train_loss_arr))\n",
        "    plt.plot(total_train_loss_arr)\n",
        "    plt.plot(total_test_loss_arr)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "F-mNTBsma4y5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2179ad9-adde-4c35-fb07-e4f89aa4f076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: loss_train = 535.4549560546875\n",
            "Step 100: loss_train = 462.6475830078125\n",
            "Step 200: loss_train = 435.9463195800781\n",
            "Step 300: loss_train = 407.0378723144531\n",
            "Step 400: loss_train = 380.9707336425781\n",
            "Step 500: loss_train = 364.1639404296875\n",
            "Step 600: loss_train = 351.3271484375\n",
            "Step 700: loss_train = 338.0254211425781\n",
            "Step 800: loss_train = 316.5018005371094\n",
            "Step 900: loss_train = 291.8456115722656\n",
            "Step 1000: loss_train = 264.1422119140625\n",
            "Step 1100: loss_train = 240.57302856445312\n",
            "Step 1200: loss_train = 220.94635009765625\n",
            "Step 1300: loss_train = 204.20314025878906\n",
            "Step 1400: loss_train = 189.89678955078125\n",
            "Step 1500: loss_train = 177.4832000732422\n",
            "Step 1600: loss_train = 166.573974609375\n",
            "Step 1700: loss_train = 156.98207092285156\n",
            "Step 1800: loss_train = 148.40914916992188\n",
            "Step 1900: loss_train = 140.76559448242188\n",
            "Step 2000: loss_train = 133.90919494628906\n",
            "Step 2100: loss_train = 127.6893310546875\n",
            "Step 2200: loss_train = 121.99879455566406\n",
            "Step 2300: loss_train = 116.78502655029297\n",
            "Step 2400: loss_train = 112.01403045654297\n",
            "Step 2500: loss_train = 107.63121032714844\n",
            "Step 2600: loss_train = 103.58154296875\n",
            "Step 2700: loss_train = 99.84523010253906\n",
            "Step 2800: loss_train = 96.39411163330078\n",
            "Step 2900: loss_train = 93.1585464477539\n",
            "Step 3000: loss_train = 90.14535522460938\n",
            "Step 3100: loss_train = 87.3293685913086\n",
            "Step 3200: loss_train = 84.6727294921875\n",
            "Step 3300: loss_train = 82.17487335205078\n",
            "Step 3400: loss_train = 79.8465805053711\n",
            "Step 3500: loss_train = 77.63883209228516\n",
            "Step 3600: loss_train = 75.5558090209961\n",
            "Step 3700: loss_train = 73.56210327148438\n",
            "Step 3800: loss_train = 71.70436096191406\n",
            "Step 3900: loss_train = 69.92823028564453\n",
            "Step 4000: loss_train = 68.24783325195312\n",
            "Step 4100: loss_train = 66.64192962646484\n",
            "Step 4200: loss_train = 65.12234497070312\n",
            "Step 4300: loss_train = 63.66213607788086\n",
            "Step 4400: loss_train = 62.28030014038086\n",
            "Step 4500: loss_train = 60.95561218261719\n",
            "Step 4600: loss_train = 59.69073486328125\n",
            "Step 4700: loss_train = 58.472686767578125\n",
            "Step 4800: loss_train = 57.310462951660156\n",
            "Step 4900: loss_train = 56.18123245239258\n",
            "Epo 0: loss_train = 594.1771850585938, loss_test = 593.8115234375, Time to run 563.238169670105\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "maml = MyModel()\n",
        "train_maml(maml, train_ds, test_ds, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOABeqX1zYpq"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('./data/fin.csv', encoding='latin-1')\n",
        "train_data = train_df.drop(columns = ['Country'])\n",
        "\n",
        "train_data_arr = []\n",
        "train_targets_arr = []\n",
        "\n",
        "test_data_arr = []\n",
        "test_target_arr = []\n",
        "r = random.randint(0,10)\n",
        "k= 0\n",
        "valueRange = 0\n",
        "\n",
        "for i in range(1,len(train_data)):\n",
        "\n",
        "  for j in range(i-1,i - valueRange - 1,-1):\n",
        "    if(j<=0):\n",
        "      continue\n",
        "    data_row = train_data[i-1:i]\n",
        "    data_row.at[i-1, 'Year'] = 1990 + j\n",
        "    if k == 11:\n",
        "      r = random.randint(0,10)\n",
        "      k=0\n",
        "    if k == r:\n",
        "      test_data_arr.append(data_row.copy())\n",
        "      target_row = train_data[j-1:j]\n",
        "      test_target_arr.append(target_row.drop(columns = ['Year']))\n",
        "      k+=1\n",
        "      continue\n",
        "    k+=1\n",
        "    train_data_arr.append(data_row.copy())\n",
        "    target_row = train_data[j-1:j]\n",
        "    train_targets_arr.append(target_row.drop(columns = ['Year']))\n",
        "\n",
        "  for j in range(i,len(train_data)):\n",
        "    data_row = train_data[i-1:i]\n",
        "    data_row.at[i-1, 'Year'] = 1990 + j\n",
        "    if k == 11:\n",
        "      r = random.randint(0,10)\n",
        "      k=0\n",
        "    if k == r:\n",
        "      test_data_arr.append(data_row.copy())\n",
        "      target_row = train_data[j:j+1]\n",
        "      test_target_arr.append(target_row.drop(columns = ['Year']))\n",
        "      k+=1\n",
        "      continue\n",
        "    k+=1\n",
        "    train_data_arr.append(data_row.copy())\n",
        "    target_row = train_data[j:j+1]\n",
        "    train_targets_arr.append(target_row.drop(columns = ['Year']))\n",
        "    if j-i > valueRange-1:\n",
        "      break\n",
        "\n",
        "train_targets = pd.concat(train_targets_arr)\n",
        "train_data = pd.concat(train_data_arr)\n",
        "\n",
        "test_targets = pd.concat(test_target_arr)\n",
        "test_data = pd.concat(test_data_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKDq8JsUKjcY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "275f7cb2-1e75-4e76-fd1f-291e6a1e0dae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5f29aeea3d91>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mscaled_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_data)\n",
        "scaled_features = scaler.transform(train_data)\n",
        "train_data = pd.DataFrame(data = scaled_features)\n",
        "\n",
        "scaled_features = scaler.transform(test_data)\n",
        "test_data = pd.DataFrame(data = scaled_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8C3htCCKlA6"
      },
      "outputs": [],
      "source": [
        "train_data_np = train_data.to_numpy()\n",
        "train_targets_np = train_targets.to_numpy()\n",
        "\n",
        "test_data_np = test_data.to_numpy()\n",
        "test_targets_np = test_targets.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Abi0EBmGDAkH"
      },
      "outputs": [],
      "source": [
        "def MiniBatches(X, Y, MiniBatchSize):\n",
        "\n",
        "    m = X.shape[0]\n",
        "    miniBatches = []\n",
        "\n",
        "    num_minibatches = m // MiniBatchSize\n",
        "    for k in range(0, num_minibatches):\n",
        "        miniBatch_X = X[k * MiniBatchSize:(k + 1) * MiniBatchSize,:]\n",
        "        miniBatch_Y = Y[k * MiniBatchSize:(k + 1) * MiniBatchSize,:]\n",
        "        miniBatch = (miniBatch_X, miniBatch_Y)\n",
        "        miniBatches.append(miniBatch)\n",
        "\n",
        "    if m % MiniBatchSize != 0:\n",
        "        miniBatch_X = X[num_minibatches * MiniBatchSize:, :]\n",
        "        miniBatch_Y = Y[num_minibatches * MiniBatchSize:, :]\n",
        "\n",
        "        miniBatch = (miniBatch_X, miniBatch_Y)\n",
        "        miniBatches.append(miniBatch)\n",
        "\n",
        "    return miniBatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3jr3-3sYByr"
      },
      "outputs": [],
      "source": [
        "test_loss_arr = []\n",
        "train_loss_arr = []\n",
        "\n",
        "def loss_function(pred_y, y):\n",
        "  return keras_backend.mean(keras.losses.MAE(y, pred_y))\n",
        "\n",
        "def compute_loss(model, x, y, loss_fn=loss_function):\n",
        "    logits = model.forward(x)\n",
        "    mae = loss_fn(logits, y)\n",
        "    return mae, logits\n",
        "\n",
        "def compute_gradients(model, x, y, loss_fn=loss_function):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss, _ = compute_loss(model, x, y, loss_fn)\n",
        "    return tape.gradient(loss, model.trainable_variables), loss\n",
        "\n",
        "\n",
        "def train_batch(x, y, model, optimizer):\n",
        "    gradients, loss = compute_gradients(model, x, y)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "def eval_test(model, optimizer, x_train, y_train, x_test, y_test, num_steps=(0, 1, 10)):\n",
        "    fit_res = []\n",
        "\n",
        "    if 0 in num_steps:\n",
        "        loss, logits = compute_loss(model, x_train, y_train)\n",
        "        print(loss)\n",
        "\n",
        "    for step in range(1, np.max(num_steps) + 1):\n",
        "        batch = MiniBatches(x_train, y_train, 1)\n",
        "        for i, t in enumerate(random.sample(range(0, len(batch)), len(batch))):\n",
        "            x_batch,y_batch = batch[t]\n",
        "            train_batch(x_batch, y_batch, model, optimizer)\n",
        "        loss_test = root_mean_squared_error(model, x_test, y_test)\n",
        "        loss_train = root_mean_squared_error(model, x_train, y_train)\n",
        "        print('Epoh {}:loss_test = {} loss_train = {}'.format(step, loss_test, loss_train))\n",
        "        test_loss_arr.append(loss_test)\n",
        "        train_loss_arr.append(loss_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "def eval_for_test(model, train_data_np, train_targets_np, test_data, test_targets,  num_steps=(0, 1, 10), lr=0.001):\n",
        "\n",
        "    copied_model = copy_model(model, train_data_np)\n",
        "\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
        "\n",
        "    model = eval_test(copied_model, optimizer, train_data_np, train_targets_np, test_data, test_targets, num_steps)\n",
        "    plt.plot(test_loss_arr)\n",
        "    plt.plot(train_loss_arr)\n",
        "    plt.show()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "def error(model):\n",
        "  pred = model.forward(train_data_np)\n",
        "  print(mean_absolute_percentage_error(pred,train_targets_np))\n",
        "  print(pred)\n",
        "  print(train_targets_np)\n",
        "  print(pred/train_targets_np)"
      ],
      "metadata": {
        "id": "cxcMqgS7y9LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM1JK01mDFY7"
      },
      "outputs": [],
      "source": [
        "model = eval_for_test(maml, train_data_np, train_targets_np, test_data_np, test_targets_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRB9LuV3gw98"
      },
      "outputs": [],
      "source": [
        "error(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка на данных за последние несколько лет"
      ],
      "metadata": {
        "id": "71q0v-f3-vpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./data/fin.csv', encoding='latin-1')\n",
        "train_data = train_df.drop(columns = ['Country']).tail(5)\n",
        "\n",
        "last_year_data_arr = []\n",
        "last_year_targets_arr = []\n",
        "\n",
        "valueRange = 0\n",
        "\n",
        "for i in range(1,len(train_data)):\n",
        "\n",
        "  for j in range(i,len(train_data)):\n",
        "    data_row = train_data.head(i).tail(1)\n",
        "    data_row.at[i+25, 'Year'] = (int)(2015 + j)\n",
        "    last_year_data_arr.append(data_row.copy())\n",
        "    target_row = train_data[j:j+1]\n",
        "    last_year_targets_arr.append(target_row.drop(columns = ['Year']))\n",
        "    if j-i > valueRange-1:\n",
        "      break\n",
        "\n",
        "last_year_targets = pd.concat(last_year_targets_arr)\n",
        "last_year_data = pd.concat(last_year_data_arr)\n",
        "\n",
        "scaler2 = MinMaxScaler()\n",
        "scaler2.fit(last_year_data)\n",
        "scaled_features2 = scaler2.transform(last_year_data)\n",
        "last_year_data = pd.DataFrame(data = scaled_features2)\n",
        "\n",
        "last_year_data_np = last_year_data.to_numpy()\n",
        "last_year_targets_np = last_year_targets.to_numpy()"
      ],
      "metadata": {
        "id": "xjIWnVf1-pqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "pred = model.forward(last_year_data_np)\n",
        "print(mean_absolute_percentage_error(pred, last_year_targets_np))\n",
        "print(pred)\n",
        "print(last_year_targets_np)\n",
        "print(pred/last_year_targets_np)"
      ],
      "metadata": {
        "id": "E61MbHwN-y7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k9kFgDTVgJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62cec72-dc8c-4ae8-dfc4-94499c563ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2019. 2238. 2388.   99.   15.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data = {'Year': [2019], 'Eggs Food': [2238.0],\n",
        "        'Eggs Production': [2388.0], 'Eggs Import Quantity': [99.0],\n",
        "       'Eggs Export Quantity': [15.0] }\n",
        "\n",
        "result = {'Year': [1999], 'Eggs Food': [1981.0],\n",
        "        'Eggs Production': [2033.0], 'Eggs Import Quantity': [9.0],\n",
        "       'Eggs Export Quantity': [7.0]}\n",
        "\n",
        "input_data = pd.DataFrame(data)\n",
        "scaled_features = scaler.transform(input_data)\n",
        "input_data = pd.DataFrame(data = scaled_features)\n",
        "input_data_np = input_data.to_numpy()\n",
        "\n",
        "input_data = pd.DataFrame(result)\n",
        "result_data = pd.DataFrame(data = input_data)\n",
        "result_data_np = result_data.to_numpy()\n",
        "\n",
        "pred = model.forward(input_data_np)\n",
        "print(input_data_np)\n",
        "#print(pred/result_data_np)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
