{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVKpLhSVYM6L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.backend as keras_backend\n",
        "\n",
        "\n",
        "# Other dependencies\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproduction\n",
        "np.random.seed(322)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpojjSDvJX1f"
      },
      "outputs": [],
      "source": [
        "class DataGenerator():\n",
        "    def __init__(self, K=10, alfa=None, beta=None):\n",
        "        self.K = K\n",
        "        self.amplitude = alfa if alfa else np.random.uniform(20.0, 50.0)/100\n",
        "        self.beta = beta if beta else np.random.uniform(1.0, 30.0)/100\n",
        "        self.sampled_points = None\n",
        "        self.x1 = self._sample()\n",
        "        self.x2 = self._sample()\n",
        "        self.x3 = self._sample()\n",
        "        self.x4 = self._sample()\n",
        "        self.x5 = self._sample()\n",
        "\n",
        "    def _sample(self):\n",
        "        return np.random.uniform(0.0, 100.0, self.K)/100\n",
        "\n",
        "    def f(self, x1, x2, x3, x4, x5):\n",
        "        y1 = x2*(1 + (x1+self.beta)*self.amplitude) #потребление\n",
        "        y2 = x3*(1 + (x1+x2+x4+x5+self.beta)*self.amplitude) #производство\n",
        "        y3 = abs(y2-y1+self.beta) #импорт\n",
        "        y4 = abs(y2+y3-y1+self.beta) #экспорт\n",
        "        return y1*1000, y2*1000, y3*10, y4*10\n",
        "\n",
        "    def batch(self, x1 = None, x2 = None, x3 = None, x4 = None, x5 = None):\n",
        "        if x1 is None:\n",
        "            x1 = self.x1\n",
        "        if x2 is None:\n",
        "            x2 = self.x2\n",
        "        if x3 is None:\n",
        "            x3 = self.x3\n",
        "        if x4 is None:\n",
        "            x4 = self.x4\n",
        "        if x5 is None:\n",
        "            x5 = self.x5\n",
        "        y1, y2, y3, y4 = self.f(x1, x2, x3, x4, x5)\n",
        "        x = []\n",
        "        y = []\n",
        "        for i in range(0,len(x1)):\n",
        "            temp_x = [x1[i], x2[i], x3[i], x4[i], x5[i]]\n",
        "            x.append(temp_x)\n",
        "        for i in  range(0,len(y1)):\n",
        "            temp_y = [y1[i], y2[i], y3[i], y4[i]]\n",
        "            y.append(temp_y)\n",
        "        return x, y\n",
        "\n",
        "    def equally_spaced_samples(self, K=None):\n",
        "        '''Returns `K` equally spaced samples.'''\n",
        "        if K is None:\n",
        "            K = self.K\n",
        "        return self.batch(\n",
        "            x1=np.linspace(1, 99, K)/100,\n",
        "            x2=np.linspace(1, 99, K)/100,\n",
        "            x3=np.linspace(1, 99, K)/100,\n",
        "            x4=np.linspace(1, 99, K)/100,\n",
        "            x5=np.linspace(1, 99, K)/100,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batches = []\n",
        "for i in range(2):\n",
        "    batches.append(DataGenerator(K=3).equally_spaced_samples())\n",
        "    (x,y) = batches[i]\n",
        "    print(np.array(x))\n",
        "    print(np.array(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uAQb_UhYeym"
      },
      "outputs": [],
      "source": [
        "class MyModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden1 = keras.layers.Dense(256, input_shape=(5,))\n",
        "        self.hidden2 = keras.layers.Dense(128)\n",
        "        self.hidden3 = keras.layers.Dense(64)\n",
        "        self.out = keras.layers.Dense(4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = keras.activations.relu(self.hidden1(x))\n",
        "        x = keras.activations.relu(self.hidden2(x))\n",
        "        x = keras.activations.relu(self.hidden3(x))\n",
        "        x = keras.activations.linear(self.out(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AknNYczD8FpB"
      },
      "outputs": [],
      "source": [
        "def copy_model(model, x):\n",
        "    copied_model = MyModel()\n",
        "    copied_model.forward(x)\n",
        "    copied_model.set_weights(model.get_weights())\n",
        "    return copied_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbUKfwhu22CK"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def root_mean_squared_error(model, x, y):\n",
        "        return K.sqrt(K.mean(K.square(model.forward(x) - y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HuLsZ8vaMHJ"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(K, train_size=10000, test_size=600):\n",
        "    def _generate_dataset(size):\n",
        "        return [DataGenerator(K=K) for _ in range(size)]\n",
        "    return _generate_dataset(train_size), _generate_dataset(test_size)\n",
        "\n",
        "train_ds, test_ds = generate_dataset(K=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6ynNxdvdoQk"
      },
      "outputs": [],
      "source": [
        "def loss_function(pred_y, y):\n",
        "  return keras_backend.mean(keras.losses.MAE(y, pred_y))\n",
        "\n",
        "def compute_loss(model, x, y, loss_fn=loss_function):\n",
        "    logits = model.forward(x)\n",
        "    mae = loss_fn(logits, y)\n",
        "    return mae, logits\n",
        "\n",
        "def train_maml(model, train_ds, test_ds, epochs, lr_inner=0.1, show_step = 100):\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    total_test_loss_arr = []\n",
        "    total_train_loss_arr = []\n",
        "    for epo in range(epochs):\n",
        "        test_loss_arr = []\n",
        "        train_loss_arr = []\n",
        "        start = time.time()\n",
        "        steps_loss = []\n",
        "        for i, t in enumerate(random.sample(range(0, len(train_ds)), len(train_ds))):\n",
        "            lr = lr_inner\n",
        "            x, y = train_ds[t].batch()\n",
        "            x = np.array(x)\n",
        "            y = np.array(y)\n",
        "\n",
        "            if len(model.get_weights()) == 0:\n",
        "                model.forward(x)\n",
        "            with tf.GradientTape() as test_tape:\n",
        "\n",
        "                with tf.GradientTape() as train_tape:\n",
        "                    loss, _ = compute_loss(model, x, y)\n",
        "                    gradients = train_tape.gradient(loss, model.trainable_variables)\n",
        "                k = 0\n",
        "                model_copy = copy_model(model, x)\n",
        "                for j in range(len(model_copy.layers)):\n",
        "                    model_copy.layers[j].kernel = tf.subtract(model.layers[j].kernel, tf.multiply(lr, gradients[k]))\n",
        "                    model_copy.layers[j].bias = tf.subtract(model.layers[j].bias, tf.multiply(lr, gradients[k+1]))\n",
        "                    k += 2\n",
        "\n",
        "                loss, _ = compute_loss(model_copy, x, y)\n",
        "\n",
        "                gradients = test_tape.gradient(loss, model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "                steps_loss.append(loss)\n",
        "                if i%show_step == 0:\n",
        "                  print('Step {}: loss_train = {}'.format(i,sum(steps_loss) / len(steps_loss)))\n",
        "\n",
        "        for i, t in enumerate(random.sample(range(0, len(train_ds)), len(train_ds))):\n",
        "            x, y = train_ds[t].batch()\n",
        "            x = np.array(x)\n",
        "            y = np.array(y)\n",
        "            test_loss = root_mean_squared_error(model, x, y)\n",
        "            train_loss_arr.append(test_loss)\n",
        "\n",
        "        for i, t in enumerate(random.sample(range(0, len(test_ds)), len(test_ds))):\n",
        "          x, y = train_ds[t].batch()\n",
        "          x = np.array(x)\n",
        "          y = np.array(y)\n",
        "          loss = root_mean_squared_error(model, x, y)\n",
        "          test_loss_arr.append(loss)\n",
        "\n",
        "\n",
        "        print('Epo {}: loss_train = {}, loss_test = {}, Time to run {}'\n",
        "        .format(epo, sum(train_loss_arr) / len(train_loss_arr), sum(test_loss_arr) / len(test_loss_arr), time.time() - start))\n",
        "        total_test_loss_arr.append(sum(test_loss_arr) / len(test_loss_arr))\n",
        "        total_train_loss_arr.append(sum(train_loss_arr) / len(train_loss_arr))\n",
        "    plt.plot(total_train_loss_arr)\n",
        "    plt.plot(total_test_loss_arr)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-mNTBsma4y5"
      },
      "outputs": [],
      "source": [
        "maml = MyModel()\n",
        "train_maml(maml, train_ds, test_ds, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOABeqX1zYpq"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('./data/fin.csv', encoding='latin-1')\n",
        "train_data = train_df.drop(columns = ['Country'])\n",
        "\n",
        "train_data_arr = []\n",
        "train_targets_arr = []\n",
        "\n",
        "test_data_arr = []\n",
        "test_target_arr = []\n",
        "r = random.randint(0,10)\n",
        "k= 0\n",
        "valueRange = 0\n",
        "\n",
        "for i in range(1,len(train_data)):\n",
        "\n",
        "  for j in range(i-1,i - valueRange - 1,-1):\n",
        "    if(j<=0):\n",
        "      continue\n",
        "    data_row = train_data[i-1:i]\n",
        "    data_row.at[i-1, 'Year'] = 1990 + j\n",
        "    if k == 11:\n",
        "      r = random.randint(0,10)\n",
        "      k=0\n",
        "    if k == r:\n",
        "      test_data_arr.append(data_row.copy())\n",
        "      target_row = train_data[j-1:j]\n",
        "      test_target_arr.append(target_row.drop(columns = ['Year']))\n",
        "      k+=1\n",
        "      continue\n",
        "    k+=1\n",
        "    train_data_arr.append(data_row.copy())\n",
        "    target_row = train_data[j-1:j]\n",
        "    train_targets_arr.append(target_row.drop(columns = ['Year']))\n",
        "\n",
        "  for j in range(i,len(train_data)):\n",
        "    data_row = train_data[i-1:i]\n",
        "    data_row.at[i-1, 'Year'] = 1990 + j\n",
        "    if k == 11:\n",
        "      r = random.randint(0,10)\n",
        "      k=0\n",
        "    if k == r:\n",
        "      test_data_arr.append(data_row.copy())\n",
        "      target_row = train_data[j:j+1]\n",
        "      test_target_arr.append(target_row.drop(columns = ['Year']))\n",
        "      k+=1\n",
        "      continue\n",
        "    k+=1\n",
        "    train_data_arr.append(data_row.copy())\n",
        "    target_row = train_data[j:j+1]\n",
        "    train_targets_arr.append(target_row.drop(columns = ['Year']))\n",
        "    if j-i > valueRange-1:\n",
        "      break\n",
        "\n",
        "train_targets = pd.concat(train_targets_arr)\n",
        "train_data = pd.concat(train_data_arr)\n",
        "\n",
        "test_targets = pd.concat(test_target_arr)\n",
        "test_data = pd.concat(test_data_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKDq8JsUKjcY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "275f7cb2-1e75-4e76-fd1f-291e6a1e0dae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5f29aeea3d91>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mscaled_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_data)\n",
        "scaled_features = scaler.transform(train_data)\n",
        "train_data = pd.DataFrame(data = scaled_features)\n",
        "\n",
        "scaled_features = scaler.transform(test_data)\n",
        "test_data = pd.DataFrame(data = scaled_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8C3htCCKlA6"
      },
      "outputs": [],
      "source": [
        "train_data_np = train_data.to_numpy()\n",
        "train_targets_np = train_targets.to_numpy()\n",
        "\n",
        "test_data_np = test_data.to_numpy()\n",
        "test_targets_np = test_targets.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Abi0EBmGDAkH"
      },
      "outputs": [],
      "source": [
        "def MiniBatches(X, Y, MiniBatchSize):\n",
        "\n",
        "    m = X.shape[0]\n",
        "    miniBatches = []\n",
        "\n",
        "    num_minibatches = m // MiniBatchSize\n",
        "    for k in range(0, num_minibatches):\n",
        "        miniBatch_X = X[k * MiniBatchSize:(k + 1) * MiniBatchSize,:]\n",
        "        miniBatch_Y = Y[k * MiniBatchSize:(k + 1) * MiniBatchSize,:]\n",
        "        miniBatch = (miniBatch_X, miniBatch_Y)\n",
        "        miniBatches.append(miniBatch)\n",
        "\n",
        "    if m % MiniBatchSize != 0:\n",
        "        miniBatch_X = X[num_minibatches * MiniBatchSize:, :]\n",
        "        miniBatch_Y = Y[num_minibatches * MiniBatchSize:, :]\n",
        "\n",
        "        miniBatch = (miniBatch_X, miniBatch_Y)\n",
        "        miniBatches.append(miniBatch)\n",
        "\n",
        "    return miniBatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3jr3-3sYByr"
      },
      "outputs": [],
      "source": [
        "test_loss_arr = []\n",
        "train_loss_arr = []\n",
        "\n",
        "def loss_function(pred_y, y):\n",
        "  return keras_backend.mean(keras.losses.MAE(y, pred_y))\n",
        "\n",
        "def compute_loss(model, x, y, loss_fn=loss_function):\n",
        "    logits = model.forward(x)\n",
        "    mae = loss_fn(logits, y)\n",
        "    return mae, logits\n",
        "\n",
        "def compute_gradients(model, x, y, loss_fn=loss_function):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss, _ = compute_loss(model, x, y, loss_fn)\n",
        "    return tape.gradient(loss, model.trainable_variables), loss\n",
        "\n",
        "\n",
        "def train_batch(x, y, model, optimizer):\n",
        "    gradients, loss = compute_gradients(model, x, y)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "def eval_test(model, optimizer, x_train, y_train, x_test, y_test, num_steps=(0, 1, 10)):\n",
        "    fit_res = []\n",
        "\n",
        "    if 0 in num_steps:\n",
        "        loss, logits = compute_loss(model, x_train, y_train)\n",
        "        print(loss)\n",
        "\n",
        "    for step in range(1, np.max(num_steps) + 1):\n",
        "        batch = MiniBatches(x_train, y_train, 1)\n",
        "        for i, t in enumerate(random.sample(range(0, len(batch)), len(batch))):\n",
        "            x_batch,y_batch = batch[t]\n",
        "            train_batch(x_batch, y_batch, model, optimizer)\n",
        "        loss_test = root_mean_squared_error(model, x_test, y_test)\n",
        "        loss_train = root_mean_squared_error(model, x_train, y_train)\n",
        "        print('Epoh {}:loss_test = {} loss_train = {}'.format(step, loss_test, loss_train))\n",
        "        test_loss_arr.append(loss_test)\n",
        "        train_loss_arr.append(loss_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "def eval_for_test(model, train_data_np, train_targets_np, test_data, test_targets,  num_steps=(0, 1, 10), lr=0.001):\n",
        "\n",
        "    copied_model = copy_model(model, train_data_np)\n",
        "\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
        "\n",
        "    model = eval_test(copied_model, optimizer, train_data_np, train_targets_np, test_data, test_targets, num_steps)\n",
        "    plt.plot(test_loss_arr)\n",
        "    plt.plot(train_loss_arr)\n",
        "    plt.show()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "def error(model):\n",
        "  pred = model.forward(train_data_np)\n",
        "  print(mean_absolute_percentage_error(pred,train_targets_np))\n",
        "  print(pred)\n",
        "  print(train_targets_np)\n",
        "  print(pred/train_targets_np)"
      ],
      "metadata": {
        "id": "cxcMqgS7y9LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM1JK01mDFY7"
      },
      "outputs": [],
      "source": [
        "model = eval_for_test(maml, train_data_np, train_targets_np, test_data_np, test_targets_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRB9LuV3gw98"
      },
      "outputs": [],
      "source": [
        "error(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка на данных за последние несколько лет"
      ],
      "metadata": {
        "id": "71q0v-f3-vpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./data/fin.csv', encoding='latin-1')\n",
        "train_data = train_df.drop(columns = ['Country']).tail(5)\n",
        "\n",
        "last_year_data_arr = []\n",
        "last_year_targets_arr = []\n",
        "\n",
        "valueRange = 0\n",
        "\n",
        "for i in range(1,len(train_data)):\n",
        "\n",
        "  for j in range(i,len(train_data)):\n",
        "    data_row = train_data.head(i).tail(1)\n",
        "    data_row.at[i+25, 'Year'] = (int)(2015 + j)\n",
        "    last_year_data_arr.append(data_row.copy())\n",
        "    target_row = train_data[j:j+1]\n",
        "    last_year_targets_arr.append(target_row.drop(columns = ['Year']))\n",
        "    if j-i > valueRange-1:\n",
        "      break\n",
        "\n",
        "last_year_targets = pd.concat(last_year_targets_arr)\n",
        "last_year_data = pd.concat(last_year_data_arr)\n",
        "\n",
        "scaler2 = MinMaxScaler()\n",
        "scaler2.fit(last_year_data)\n",
        "scaled_features2 = scaler2.transform(last_year_data)\n",
        "last_year_data = pd.DataFrame(data = scaled_features2)\n",
        "\n",
        "last_year_data_np = last_year_data.to_numpy()\n",
        "last_year_targets_np = last_year_targets.to_numpy()"
      ],
      "metadata": {
        "id": "xjIWnVf1-pqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "pred = model.forward(last_year_data_np)\n",
        "print(mean_absolute_percentage_error(pred, last_year_targets_np))\n",
        "print(pred)\n",
        "print(last_year_targets_np)\n",
        "print(pred/last_year_targets_np)"
      ],
      "metadata": {
        "id": "E61MbHwN-y7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k9kFgDTVgJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62cec72-dc8c-4ae8-dfc4-94499c563ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2019. 2238. 2388.   99.   15.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data = {'Year': [2019], 'Eggs Food': [2238.0],\n",
        "        'Eggs Production': [2388.0], 'Eggs Import Quantity': [99.0],\n",
        "       'Eggs Export Quantity': [15.0] }\n",
        "\n",
        "result = {'Year': [1999], 'Eggs Food': [1981.0],\n",
        "        'Eggs Production': [2033.0], 'Eggs Import Quantity': [9.0],\n",
        "       'Eggs Export Quantity': [7.0]}\n",
        "\n",
        "input_data = pd.DataFrame(data)\n",
        "scaled_features = scaler.transform(input_data)\n",
        "input_data = pd.DataFrame(data = scaled_features)\n",
        "input_data_np = input_data.to_numpy()\n",
        "\n",
        "input_data = pd.DataFrame(result)\n",
        "result_data = pd.DataFrame(data = input_data)\n",
        "result_data_np = result_data.to_numpy()\n",
        "\n",
        "pred = model.forward(input_data_np)\n",
        "print(input_data_np)\n",
        "#print(pred/result_data_np)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}